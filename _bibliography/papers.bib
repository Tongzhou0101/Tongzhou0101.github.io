---
---

@string{aps = {American Physical Society,}}

@article{li2024neural,
  abbr={Frontiers},
  title={Neural architecture search for adversarial robustness via learnable pruning},
  author={Li, Yize and Zhao, Pu and Ding, Ruyi and Zhou, Tong and Fei, Yunsi and Xu, Xiaolin and Lin, Xue},
  journal={Frontiers in High Performance Computing},
  volume={2},
  pages={1301384},
  year={2024},
  pdf={https://www.frontiersin.org/journals/high-performance-computing/articles/10.3389/fhpcp.2024.1301384/full}
}


@inproceedings{zhou2024adapi,
  abbr={ICCAD},
  title={AdaPI: Facilitating dnn model adaptivity for efficient private inference in edge computing},
  author={Zhou, Tong and Zhao, Jiahui and Luo, Yukui and Xie, Xi and Wen, Wujie and Ding, Caiwen and Xu, Xiaolin},
  pdf={https://arxiv.org/pdf/2407.05633},
  booktitle={2024 IEEE/ACM International Conference on Computer Aided Design (ICCAD)},
  year={2024}
}


@inproceedings{zhou2024bileve,
  abbr={NeurIPS},
  title={Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature},
  author={Zhou, Tong and Zhao, Xuandong and Xu, Xiaolin and Ren, Shaolei},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  selected={true},
  abstract={Text watermarks for large language models (LLMs) have been commonly used
  to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing
  watermarking techniques typically prioritize robustness against removal attacks,
  unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly
  alter the meanings of LLM-generated responses or even forge harmful content,
  potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature
  bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained
  signal to trace text sources when the signature is invalid (enhancing detectability)
  via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during
  detection, reliably tracing text provenance and regulating LLMs. The experiments
  conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve
  in defeating spoofing attacks with enhanced detectability.},
  url={https://arxiv.org/pdf/2406.01946}
}

@inproceedings{liu2024tbnet,
  abbr={DAC},
  title={TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments},
  author={Liu, Ziyu and Zhou, Tong and Luo, Yukui and Xu, Xiaolin},
  booktitle={Proceedings of the 61st ACM/IEEE Design Automation Conference},
  pdf={https://arxiv.org/pdf/2405.03974},
  year={2024}
}

@inproceedings{
zhou2024archlock,
abbr={ICLR},
title={ArchLock: Locking {DNN} Transferability at the Architecture Level with a Zero-Cost Binary Predictor},
author={Zhou, Tong and Ren, Shaolei and Xu, Xiaolin},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
pdf={https://openreview.net/pdf?id=e2YOVTenU9},
code={https://github.com/Tongzhou0101/ArchLock},
abstract={Deep neural network (DNN) models are vulnerable to misuse by attackers who try to adapt them to other tasks. Existing defenses focus on model parameters, neglecting architectural-level defenses. This paper introduces ArchLock, a method utilizing neural architecture search (NAS) and zero-cost proxies to generate models with low transferability, hindering attackers' attempts. ArchLock maintains high performance on the original task while minimizing performance on potential target tasks.},
selected={true}
}

@inproceedings{liu2023mirrornet,
  abbr={ICCAD},
  title={MirrorNet: A TEE-Friendly Framework for Secure On-Device DNN Inference},
  author={Liu, Ziyu and Luo, Yukui and Duan, Shijin and Zhou, Tong and Xu, Xiaolin},
  booktitle={2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)},
  pages={1--9},
  year={2023},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323746},
  organization={IEEE}
}

@inproceedings{peng2023autorep,
  abbr={ICCV},
  title={Autorep: Automatic relu replacement for fast private network inference},
  author={Peng*, Hongwu and Huang*, Shaoyi and Zhou*, Tong  and  Luo, Yukui and Wang, Chenghong and Wang, Zigeng and Zhao, Jiahui and Xie, Xi and Li, Ang and Geng, Tony and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5178--5188},
  pdf={https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_AutoReP_Automatic_ReLU_Replacement_for_Fast_Private_Network_Inference_ICCV_2023_paper.pdf},
  code={https://github.com/HarveyP123/AutoReP},
  year={2023}
}

@InProceedings{pmlr-v202-zhou23h,
  abbr={ICML},
  title = 	 {{NNS}plitter: An Active Defense Solution for {DNN} Model via Automated Weight Obfuscation},
  author =       {Zhou, Tong and Luo, Yukui and Ren, Shaolei and Xu, Xiaolin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {42614--42624},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = {https://proceedings.mlr.press/v202/zhou23h/zhou23h.pdf},
  code = {https://github.com/Tongzhou0101/NNSplitter},
  abstract={NNSplitter is a novel IP protection scheme for DNN models, dividing them into an obfuscated portion stored in normal memory and model secrets stored in secure memory. The obfuscated model, with perturbed weights, hampers attackers' efforts, while authorized users accessing secure memory achieve high performance. This approach ensures protection against adaptive attacks, maintaining security for DNN models.},
  selected={true}
}

@inproceedings{zhou2022obfunas,
  abbr={ICCAD},
  title={ObfuNAS: A Neural Architecture Search-based DNN Obfuscation Approach <span style="color: #c00;">(Best Paper Nomination)</span>},
  author={Zhou, Tong and Ren, Shaolei and Xu, Xiaolin},
  award={Best Paper Nomination},
  booktitle={Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
  pages={1--9},
  year={2022},
  code = {https://github.com/Tongzhou0101/ObfuNAS},
  pdf = {https://arxiv.org/pdf/2208.08569.pdf},
  abstract= {ObfuNAS tackles the threat of malicious architecture extraction in DNN security. By converting architecture obfuscation into a neural architecture search problem, it ensures that obfuscated models perform worse than the original. },
  selected={true}
}

@inproceedings{zhou2021deep,
  abbr={NANOARCH},
  title={Deep neural network security from a hardware perspective},
  author={Zhou, Tong and Zhang, Yuheng and Duan, Shijin and Luo, Yukui and Xu, Xiaolin},
  booktitle={2021 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)},
  pages={1--6},
  year={2021},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9642246},
  organization={IEEE}
}